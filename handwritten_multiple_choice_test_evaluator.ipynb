{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "NwhaA5O4QsDv",
        "61Yx4uiBQ4K0",
        "JeW50SezRA9U"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# HANDWRITTEN MULTIPLE-CHOICE TEST EVALUATOR\n",
        "An implementation of automated handwritten multiple-choice test evaluation, employing OpenCV for image preprocessing and PyTorch for training a convolutional neural network on the EMNIST dataset for image classification."
      ],
      "metadata": {
        "id": "9z2oy94RRxJa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## IMPORT MODULES AND SET GLOBALS"
      ],
      "metadata": {
        "id": "EaklWdMZRwYU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5Zz7NinfDDM",
        "outputId": "87871fa6-255e-4976-a312-6da52cd125ab"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.10/dist-packages (4.2.0)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (23.2.1)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.2.2)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.10/dist-packages (from gradio) (0.104.1)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.10/dist-packages (from gradio) (0.3.1)\n",
            "Requirement already satisfied: gradio-client==0.7.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.7.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.19.0)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.1.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.2)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.3)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: numpy~=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.23.5)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.9.10)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (23.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.5.3)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (9.4.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.4.2)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart in /usr/local/lib/python3.10/dist-packages (from gradio) (0.0.6)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.1)\n",
            "Requirement already satisfied: requests~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.31.0)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: tomlkit==0.12.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.0)\n",
            "Requirement already satisfied: typer[all]<1.0,>=0.9 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.9.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.8.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.24.0.post1)\n",
            "Requirement already satisfied: websockets<12.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (11.0.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==0.7.0->gradio) (2023.6.0)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.12.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.14.0->gradio) (3.13.1)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.14.0->gradio) (4.66.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (4.44.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2023.3.post1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.10.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.10.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests~=2.0->gradio) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests~=2.0->gradio) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests~=2.0->gradio) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests~=2.0->gradio) (2023.7.22)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer[all]<1.0,>=0.9->gradio) (8.1.7)\n",
            "Requirement already satisfied: colorama<0.5.0,>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from typer[all]<1.0,>=0.9->gradio) (0.4.6)\n",
            "Requirement already satisfied: shellingham<2.0.0,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer[all]<1.0,>=0.9->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich<14.0.0,>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer[all]<1.0,>=0.9->gradio) (13.6.0)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.14.0->gradio) (0.14.0)\n",
            "Requirement already satisfied: anyio<4.0.0,>=3.7.1 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio) (3.7.1)\n",
            "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio) (0.27.0)\n",
            "Requirement already satisfied: httpcore in /usr/local/lib/python3.10/dist-packages (from httpx->gradio) (1.0.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->gradio) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0.0,>=3.7.1->fastapi->gradio) (1.1.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (23.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (2023.7.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.30.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.12.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For downloading model weights\n",
        "import os\n",
        "\n",
        "# For image preprocessing\n",
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# For the image classifier\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# For the interface\n",
        "import gradio as gr"
      ],
      "metadata": {
        "id": "_4U_dUaQs2i-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Globals:\n",
        "# Define constants\n",
        "PREDICTION_TO_STRING = [\"A\", \"B\", \"C\", \"D\"]\n",
        "\n",
        "# Define variables\n",
        "# For preprocessing\n",
        "image_width = 1125\n",
        "image_height = 1500\n",
        "thresh_block_size = 11\n",
        "thresh_constant = 5\n",
        "line_length = 5000\n",
        "\n",
        "# For ROI extraction\n",
        "min_contour_width = 15\n",
        "min_contour_height = 15\n",
        "max_contour_width = 100\n",
        "max_contour_height = 100\n",
        "y_epsilon = 25\n",
        "number_of_columns = 2"
      ],
      "metadata": {
        "id": "RrOzL7g0zab_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## THE CLASSIFIER\n",
        "Prepare the ABCD Classifier Model."
      ],
      "metadata": {
        "id": "TFv3JBjAOy08"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "yK9WDGQQoKOA"
      },
      "outputs": [],
      "source": [
        "# Download the model's state dictionary from repository\n",
        "GITHUB_URL = \"https://raw.githubusercontent.com/CorvusBrachyrhynchos/Handwritten-Multiple-Choice-Test-Corrector/main/abcd_classifier.pth\"\n",
        "MODEL_PATH = \"/content/model.pth\"\n",
        "os.system(f\"wget {GITHUB_URL} -O {MODEL_PATH}\")\n",
        "model_state_dict = torch.load(MODEL_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model class\n",
        "class ABCDClassifier(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(ABCDClassifier, self).__init__()\n",
        "\n",
        "    self.conv1 = nn.Conv2d(1, 16, kernel_size=5)\n",
        "    self.conv2 = nn.Conv2d(16, 32, kernel_size=5)\n",
        "    self.conv2_drop = nn.Dropout2d()\n",
        "    self.fc1 = nn.Linear(32 * 4 * 4, 128)\n",
        "    self.fc2 = nn.Linear(128, 4)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "    x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
        "    x = x.view(-1, 32 * 4 * 4)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.dropout(x, training=self.training)\n",
        "    x = self.fc2(x)\n",
        "\n",
        "    return F.softmax(x, dim=1)"
      ],
      "metadata": {
        "id": "avmc5ro9s1s1"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model\n",
        "model = ABCDClassifier()\n",
        "model.load_state_dict(model_state_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o3XUKncaOrDJ",
        "outputId": "64b797d9-d8dc-412c-87a0-1ccbcae32806"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## THE FUNCTIONS\n",
        "Define the functions for interfacing, image preprocessing, region of interest (ROI) extraction, and image classification."
      ],
      "metadata": {
        "id": "qsWpajjjOtmT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Interface Functions"
      ],
      "metadata": {
        "id": "7Q5HWgmSQx4t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The main function used for the interface.\n",
        "# Takes in an array of strings (image paths), and an array of chars (correction key)\n",
        "# Returns three arrays: image scores, score analysis, item analysis\n",
        "def correct_image_set(image_paths_array, correction_key):\n",
        "  image_scores = [] # Length is len(image_paths_array)\n",
        "  score_analysis = [0] * (len(correction_key) + 1)\n",
        "  item_analysis = [0] * len(correction_key)\n",
        "\n",
        "  print(f\"CORRECTION KEY: {correction_key}\")\n",
        "\n",
        "  for i in range(len(image_paths_array)):\n",
        "    correct_image(image_paths_array[i], image_scores, score_analysis, item_analysis, correction_key, i + 1)\n",
        "\n",
        "  print(\"\\nAFTER EVALUATION AND ANALYSIS\")\n",
        "  print(f\"Image Scores: {image_scores}\")\n",
        "  print(f\"Score Analysis: {score_analysis}\")\n",
        "  print(f\"Item Analysis: {item_analysis}\")\n",
        "\n",
        "  return image_scores, score_analysis, item_analysis\n",
        "\n",
        "\n",
        "# A helper function for readability\n",
        "# Takes in an image path, the three arrays to be modified, an array of chars, and an integer\n",
        "# Void return value, modifies the three arrays directly\n",
        "def correct_image(img_path, img_scr_arr, scr_ana_arr, itm_ana_arr, correction_key, student_number):\n",
        "  score = 0\n",
        "  item_answers = [] # An array of strings\n",
        "\n",
        "  # Begin processing for ROI extraction\n",
        "  base_image = cv2.imread(img_path)\n",
        "  resized_image = cv2.resize(base_image, (image_width, image_height))\n",
        "  image_for_contour_finding, ocr_image, processing_error = preprocess(resized_image)\n",
        "  if processing_error: # Check for exception\n",
        "    invalid_num_of_items(img_scr_arr)\n",
        "    return\n",
        "\n",
        "  rois, extraction_error = extract_rois(image_for_contour_finding, ocr_image, len(correction_key))\n",
        "  if extraction_error: # Check for exception\n",
        "    invalid_num_of_items(img_scr_arr)\n",
        "    return\n",
        "\n",
        "  # Classify the ROIs\n",
        "  for roi in rois:\n",
        "    item_answers.append(classify_roi(roi))\n",
        "\n",
        "  print(f\"\\nSTUDENT {student_number}:\")\n",
        "  print(f\"Item Answers ({len(item_answers)}): {item_answers}\\n\")\n",
        "\n",
        "  if len(item_answers) != len(correction_key): # Check for exception (extra layer of safety)\n",
        "    invalid_num_of_items(img_scr_arr)\n",
        "    return\n",
        "\n",
        "  # Evaluate and analyze\n",
        "  for i in range(len(correction_key)):\n",
        "    if item_answers[i] == correction_key[i] or correction_key[i] == \"X\":\n",
        "      score += 1\n",
        "      itm_ana_arr[i] += 1\n",
        "  img_scr_arr.append(score)\n",
        "  scr_ana_arr[score] += 1\n",
        "\n",
        "\n",
        "# Function for error logic: sets the score to -1 (-1 score means invalid image)\n",
        "# Takes in the array to be modified (score array)\n",
        "# Void return value, modifies the array directly\n",
        "def invalid_num_of_items(score_array):\n",
        "  score_array.append(-1)\n",
        "  print(\"INVALID: NUM OF ITEM ANSWERS != NUM OF ITEMS IN CORRECTION KEY\")"
      ],
      "metadata": {
        "id": "UaVWpmkRtXFN"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Image Preprocessing Functions"
      ],
      "metadata": {
        "id": "NwhaA5O4QsDv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Takes in an image (the base image)\n",
        "# Returns an image made for contour detection, an image for OCR, and a boolean for error handling\n",
        "def preprocess(input_image):\n",
        "  # ------------------------------------ FOR OCR IMAGE------------------------------------ #\n",
        "  gray = cv2.cvtColor(input_image, cv2.COLOR_BGR2GRAY)\n",
        "  threshed_gray = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, thresh_block_size, thresh_constant)\n",
        "  opened = cv2.bitwise_not(cv2.morphologyEx(cv2.bitwise_not(threshed_gray), cv2.MORPH_OPEN, np.ones((2,2),np.uint8), iterations=1))\n",
        "  image_for_ocr = remove_lines(opened)\n",
        "\n",
        "  # ----------------------------------- FOR HEADER MASK ----------------------------------- #\n",
        "  blur = cv2.GaussianBlur(gray, (9,9), 0)\n",
        "  threshed_blur = cv2.adaptiveThreshold(blur, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, thresh_block_size, thresh_constant)\n",
        "  removed_lines = remove_lines(threshed_blur, 6)\n",
        "  intensely_dilated = cv2.dilate(cv2.bitwise_not(removed_lines), cv2.getStructuringElement(cv2.MORPH_RECT, (2000, 40)), iterations=1)\n",
        "\n",
        "  # Determine divider y positions\n",
        "  black_ys = np.where(np.any(intensely_dilated == 0, axis=1))[0].tolist()\n",
        "  to_be_removed = []\n",
        "  for i in range(1, len(black_ys)):\n",
        "    if black_ys[i] - black_ys[i - 1] == 1:\n",
        "      to_be_removed.append(i)\n",
        "  stripe_positions = [black_ys[idx] for idx in range(len(black_ys)) if not idx in to_be_removed]\n",
        "\n",
        "  # Determine which stripe is the header border\n",
        "  num_of_stripe_positions = len(stripe_positions)\n",
        "  header_border = 0\n",
        "  match num_of_stripe_positions:\n",
        "    case 1:\n",
        "      header_border = stripe_positions[0]\n",
        "    case 2:\n",
        "      header_border = stripe_positions[0] if stripe_positions[1] > 500 else stripe_positions[1]\n",
        "    case _:\n",
        "      found = False\n",
        "      for stripe_position in stripe_positions:\n",
        "        if stripe_position > 100 and stripe_position < 500:\n",
        "          header_border = stripe_position\n",
        "          found = True\n",
        "          continue\n",
        "      if not found: # If there is no stripe, consider the image invalid\n",
        "        return [0], [0], True\n",
        "\n",
        "  # Create a mask based on header border\n",
        "  mask = np.ones(intensely_dilated.shape, dtype=np.uint8) * 255\n",
        "  mask[header_border:, :] = 0\n",
        "\n",
        "  # ---------------------------------- FOR CONTOUR IMAGE ---------------------------------- #\n",
        "  masked_removed_lines = cv2.bitwise_or(removed_lines, mask)\n",
        "  dilated = cv2.dilate(cv2.bitwise_not(masked_removed_lines), cv2.getStructuringElement(cv2.MORPH_RECT, (6, 6)), iterations=1)\n",
        "  image_for_countour = cv2.Canny(dilated, 100, 200)\n",
        "  return image_for_countour, image_for_ocr, False\n",
        "\n"
      ],
      "metadata": {
        "id": "JzWGfdPG58Cb"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A helper function for the preprocess function\n",
        "# Takes in an image, and optionally line thickness (an int that determines how erased the notepad lines are)\n",
        "# Returns a modified image, where the notepad lines are attempted to be erased\n",
        "def remove_lines(input_image, thickness=2):\n",
        "  edges = cv2.Canny(input_image, 200, 200)\n",
        "  lines = cv2.HoughLines(edges, 1, np.pi / 180, 200)\n",
        "\n",
        "  # Creates a mask based on the lines found by HoughLines\n",
        "  line_mask = np.zeros_like(input_image)\n",
        "  for line in lines:\n",
        "    rho, theta = line[0]\n",
        "    if theta < 1.0 or theta > 2.0:\n",
        "      continue\n",
        "    a = np.cos(theta)\n",
        "    b = np.sin(theta)\n",
        "    x0 = a * rho\n",
        "    y0 = b * rho\n",
        "    x1 = int(x0 + line_length * (-b))\n",
        "    y1 = int(y0 + line_length * (a))\n",
        "    x2 = int(x0 - line_length * (-b))\n",
        "    y2 = int(y0 - line_length * (a))\n",
        "    cv2.line(line_mask, (x1, y1), (x2, y2), (255, 255, 255), 2)\n",
        "\n",
        "  # Dilates the lines vertically based on thickness parameter\n",
        "  dilation_kernel = np.ones((thickness, 1),np.uint8)\n",
        "  line_mask = cv2.dilate(line_mask, dilation_kernel, iterations=1)\n",
        "\n",
        "  # Subtracts the mask from the base image and applies MORPH_OPEN to denoise\n",
        "  sub_result = cv2.bitwise_or(input_image, line_mask)\n",
        "  final_result = cv2.bitwise_not(cv2.morphologyEx(cv2.bitwise_not(sub_result), cv2.MORPH_OPEN, np.ones((2, 2),np.uint8), iterations=1))\n",
        "  return final_result"
      ],
      "metadata": {
        "id": "oxn1iQUX7be6"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ROI Extraction Function"
      ],
      "metadata": {
        "id": "61Yx4uiBQ4K0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Takes in an image from which contours are found, an image for OCR, and target number of ROIs\n",
        "# Returns an array of RIOs (of shape [y, x]), and a boolean for error handling\n",
        "def extract_rois(contour_image, ocr_image, num_of_items):\n",
        "  # -------------------------------- FOR GETTING CONTOURS -------------------------------- #\n",
        "  all_contours = cv2.findContours(contour_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "  all_contours = all_contours[0] if len(all_contours) == 2 else all_contours[1]\n",
        "  all_contours = sorted(all_contours, key=lambda x: (cv2.boundingRect(x)[1], cv2.boundingRect(x)[0]))\n",
        "\n",
        "  # Filter contours by size\n",
        "  filtered_contours = []\n",
        "  x_coordinates = []\n",
        "  for contour in all_contours:\n",
        "    x, y, w, h = cv2.boundingRect(contour)\n",
        "    if h > min_contour_height and w > min_contour_width and h < max_contour_height and w < max_contour_width:\n",
        "      filtered_contours.append(contour)\n",
        "      x_coordinates.append(int(x + (w / 2)))\n",
        "\n",
        "  # -------------------------------- FOR GROUPING CONTOURS -------------------------------- #\n",
        "  # Group contours based on x position\n",
        "  x_coordinates = np.array(x_coordinates).reshape(-1, 1)\n",
        "\n",
        "  # Perform K-means clustering with k = number_of_columns\n",
        "  optimal_k = number_of_columns\n",
        "  kmeans = KMeans(n_clusters=optimal_k, init='k-means++', max_iter=300, n_init=10, random_state=0)\n",
        "  kmeans.fit(x_coordinates)\n",
        "\n",
        "  # Group the contours based on the cluster assignments\n",
        "  grouped_contours = [[] for _ in range(optimal_k)]\n",
        "  for i, label in enumerate(kmeans.labels_):\n",
        "    grouped_contours[label].append(filtered_contours[i])\n",
        "  grouped_contours = sorted(grouped_contours, key=lambda group: cv2.boundingRect(group[0])[0])\n",
        "\n",
        "  # ---------------------------------- FOR GETTING ROIs ---------------------------------- #\n",
        "  # Get a list of ROIs to filter non-letters based on relative position\n",
        "  roi_x_dict = {}\n",
        "  roi_y_dict = {}\n",
        "  roi_boxes = {}\n",
        "  counter = 0\n",
        "  for group in grouped_contours:\n",
        "    for countour in group:\n",
        "      counter += 1\n",
        "      x, y, w, h = cv2.boundingRect(countour)\n",
        "      roi_x_dict[counter] = x\n",
        "      roi_y_dict[counter] = y\n",
        "      roi_boxes[counter] = (x, y, w, h)\n",
        "\n",
        "  # Filter ROIs by Y position, compared to nearby ROIs (prev, prev-prev, next)\n",
        "  duplicate_dict = roi_y_dict.copy()\n",
        "  for key in duplicate_dict:\n",
        "    prev_prev_key = key - 2\n",
        "    prev_key = key - 1\n",
        "    next_key = key + 1\n",
        "    if not key in roi_y_dict:\n",
        "      continue\n",
        "\n",
        "    if prev_key in roi_y_dict:\n",
        "      if abs(duplicate_dict[prev_key] - duplicate_dict[key]) <= y_epsilon:\n",
        "        prev_key_x = roi_x_dict[prev_key]\n",
        "        curr_key_x = roi_x_dict[key]\n",
        "        if prev_key_x > curr_key_x:\n",
        "          if key in roi_y_dict:\n",
        "            roi_y_dict.pop(key)\n",
        "            continue\n",
        "\n",
        "    if prev_prev_key in roi_y_dict:\n",
        "      if abs(duplicate_dict[prev_prev_key] - duplicate_dict[key]) <= y_epsilon:\n",
        "        prev_prev_key_x = roi_x_dict[prev_prev_key]\n",
        "        curr_key_x = roi_x_dict[key]\n",
        "        if prev_prev_key_x > curr_key_x:\n",
        "          if key in roi_y_dict:\n",
        "            roi_y_dict.pop(key)\n",
        "            continue\n",
        "\n",
        "    if next_key in roi_y_dict:\n",
        "      if abs(duplicate_dict[next_key] - duplicate_dict[key]) <= y_epsilon:\n",
        "        next_key_x = roi_x_dict[next_key]\n",
        "        curr_key_x = roi_x_dict[key]\n",
        "        if next_key_x > curr_key_x:\n",
        "          if key in roi_y_dict:\n",
        "            roi_y_dict.pop(key)\n",
        "            continue\n",
        "\n",
        "  # Handle exception: If the number of ROIs found is not the same as the target, consider the image invalid\n",
        "  if len(roi_y_dict) != num_of_items:\n",
        "    return [0], True\n",
        "\n",
        "  # Generate the array of ROIs\n",
        "  rois = []\n",
        "  counter = 0\n",
        "  for roi in roi_y_dict:\n",
        "    if roi in roi_boxes:\n",
        "      x, y, w, h = roi_boxes[roi]\n",
        "      counter += 1\n",
        "      roi_img = ocr_image[y:y+h, x:x+w]\n",
        "      rois.append(roi_img)\n",
        "\n",
        "  return rois, False"
      ],
      "metadata": {
        "id": "Oq6oUFgZ7sfw"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Image Classification Function"
      ],
      "metadata": {
        "id": "JeW50SezRA9U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Takes in an ROI expressed as a numpy array of shape [y, x]\n",
        "# Returns the classifiers classification of the ROI\n",
        "def classify_roi(roi):\n",
        "  # Preprocess ROI\n",
        "  roi = cv2.resize(roi, (28, 28)) # Resize to fit model requirement\n",
        "  roi = cv2.bitwise_not(roi) # Invert to fit model requirement\n",
        "  roi = roi / 255.0 # Normalize\n",
        "\n",
        "  roi = torch.from_numpy(roi) # Convert to tensor\n",
        "  roi = roi.view(1, 1, 28, 28) # Reshape to fit model requirement\n",
        "  roi = roi.to(torch.float32) # Change data type to fit model requirement\n",
        "\n",
        "  # Classify the ROI\n",
        "  model.eval()\n",
        "  output = model(roi)\n",
        "  prediction = output.argmax(dim=1, keepdim=True).item() # Returns a number from 0 to 3\n",
        "  predicted_letter = PREDICTION_TO_STRING[prediction] # Converts the number to the corresponding letter\n",
        "  return predicted_letter"
      ],
      "metadata": {
        "id": "LWsrH1MTRBnX"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## THE INTERFACE\n",
        "Use Gradio to create a user-friendly interface."
      ],
      "metadata": {
        "id": "aolbZpscRhXM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the sample images for the demo interface\n",
        "SAMPLE_IMAGES_FOLDER_URL = \"https://raw.githubusercontent.com/CorvusBrachyrhynchos/Handwritten-Multiple-Choice-Test-Corrector/main/sample_images/\"\n",
        "IMAGE_SAVE_PATH = \"/content/\"\n",
        "\n",
        "NUM_OF_SAMPLES = 2\n",
        "for i in range(1, NUM_OF_SAMPLES + 1):\n",
        "  suffix = f\"sample_{i}.jpg\"\n",
        "  os.system(f\"wget {SAMPLE_IMAGES_FOLDER_URL}{suffix} -O {IMAGE_SAVE_PATH}{suffix}\")"
      ],
      "metadata": {
        "id": "1_LR6TyoZxVJ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the interface function\n",
        "def interface_function(correction_key_string, *images):\n",
        "\n",
        "  # Set correction key\n",
        "  correction_key = []\n",
        "  for item in correction_key_string:\n",
        "    if item in [\"A\", \"B\", \"C\", \"D\", \"X\"]:\n",
        "      correction_key.append(item.upper())\n",
        "\n",
        "  # Run the main function\n",
        "  image_scores, score_analysis, item_analysis = correct_image_set(images[0], correction_key)\n",
        "\n",
        "  # Generate output strings\n",
        "  scores_string = \"\"\n",
        "  score_analysis_string = \"\"\n",
        "  item_analysis_string = \"\"\n",
        "\n",
        "  num_of_items = len(correction_key)\n",
        "  num_of_students = len(images[0])\n",
        "\n",
        "  for index, score in enumerate(image_scores, start=1):\n",
        "    scores_string += f\"Student {index}: {score}/{num_of_items}\\n\"\n",
        "\n",
        "  for index, num_of_scores in enumerate(score_analysis):\n",
        "    score_analysis_string += f\"{index}/{num_of_items}: {num_of_scores}\\n\"\n",
        "\n",
        "  for index, item_score in enumerate(item_analysis, start=1):\n",
        "    item_analysis_string += f\"Item {index}: {item_score}/{num_of_students}\\n\"\n",
        "\n",
        "  return scores_string, score_analysis_string, item_analysis_string\n",
        "\n",
        "\n",
        "# Define the Gradio interface\n",
        "\n",
        "# Set the description string\n",
        "desc ='''\n",
        "This is the demo interface for the research project titled \"AUTOMATING MULTIPLE-CHOICE HANDWRITTEN TEST CORRECTION USING COMPUTER VISION AND DEEP LEARNING\" by Edradan, G., Serrano, D., and Tunguia, T.\n",
        "\n",
        "Instructions:\n",
        "\n",
        "First Input: Enter the correction key. It must be a continuous string of text, with each letter representing the correct answer for each item in consecutive order. The only letters accepted are A, B, C, and D, but an X can be written to represent an item that would accept any answer (bonus item).\n",
        "\n",
        "Second Input: Upload the images of the papers to be evaluated and analyzed. The order of evaluation is based on the order in which the images are uploaded.\n",
        "\n",
        "For better results, the following are recommended:\n",
        "\n",
        "Regarding the documents to be evaluated:\n",
        "  - Have substantial left and right margins\n",
        "  - Provide a blank line between the header and the answers\n",
        "  - Write the answers in capitals and in two columns\n",
        "  - Avoid having the answers overlap with the notepad lines\n",
        "  - Have significant space between the numbers and the letters\n",
        "  - Write the item numbers smaller than the letters\n",
        "\n",
        "Regarding the photo:\n",
        "  - Have an aspect ratio of 3:4\n",
        "  - Have a resolution of at least 1125 px by 1500 px\n",
        "  - Have adequate lighting; use flash if necessary\n",
        "\n",
        "'''\n",
        "\n",
        "interface = gr.Interface(\n",
        "    fn=interface_function,\n",
        "    title=\"HANDWRITTEN MULTIPLE-CHOICE TEST EVALUATOR\",\n",
        "    description=desc,\n",
        "    allow_flagging=\"never\",\n",
        "    inputs=[gr.Textbox(label=\"Correction Key\", placeholder=\"ABCDABCDABCD\"),\n",
        "            gr.File(file_count=\"multiple\", file_types=[\".jpg\"], label=\"Upload Image(s)\")],\n",
        "    outputs=[gr.Textbox(label=\"Scores\"),\n",
        "             gr.Textbox(label=\"Score Analysis\"),\n",
        "             gr.Textbox(label=\"Item Analysis\")],\n",
        "    examples=[\n",
        "        [\"ABCDXABCDABCDABCDABCDABCDABCDABCDABCDABCDABCDABCDA\",(['sample_1.jpg'])],\n",
        "        [\"ABCDXABCDABCDABCDABCDABCDABCDABCDABCDABCDABCDABCDA\",(['sample_2.jpg'])],\n",
        "        [\"ABCDXABCDABCDABCDABCDABCDABCDABCDABCDABCDABCDABCDA\",(['sample_1.jpg', 'sample_2.jpg'])],\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "# Launch the interface\n",
        "interface.launch(debug=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 626
        },
        "id": "AnBH_XUARhyV",
        "outputId": "58d49176-d3a9-4ac5-da0b-d0c8b7ad2957"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://652218cd1d9a64d2c0.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://652218cd1d9a64d2c0.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    }
  ]
}